{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:ISLR2':\n",
      "\n",
      "    Boston\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(ISLR2)\n",
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda                    package:MASS                    R Documentation\n",
      "\n",
      "_\bL_\bi_\bn_\be_\ba_\br _\bD_\bi_\bs_\bc_\br_\bi_\bm_\bi_\bn_\ba_\bn_\bt _\bA_\bn_\ba_\bl_\by_\bs_\bi_\bs\n",
      "\n",
      "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
      "\n",
      "     Linear discriminant analysis.\n",
      "\n",
      "_\bU_\bs_\ba_\bg_\be:\n",
      "\n",
      "     lda(x, ...)\n",
      "     \n",
      "     ## S3 method for class 'formula'\n",
      "     lda(formula, data, ..., subset, na.action)\n",
      "     \n",
      "     ## Default S3 method:\n",
      "     lda(x, grouping, prior = proportions, tol = 1.0e-4,\n",
      "         method, CV = FALSE, nu, ...)\n",
      "     \n",
      "     ## S3 method for class 'data.frame'\n",
      "     lda(x, ...)\n",
      "     \n",
      "     ## S3 method for class 'matrix'\n",
      "     lda(x, grouping, ..., subset, na.action)\n",
      "     \n",
      "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
      "\n",
      " formula: A formula of the form 'groups ~ x1 + x2 + ...'  That is, the\n",
      "          response is the grouping factor and the right hand side\n",
      "          specifies the (non-factor) discriminators.\n",
      "\n",
      "    data: An optional data frame, list or environment from which\n",
      "          variables specified in 'formula' are preferentially to be\n",
      "          taken.\n",
      "\n",
      "       x: (required if no formula is given as the principal argument.)\n",
      "          a matrix or data frame or Matrix containing the explanatory\n",
      "          variables.\n",
      "\n",
      "grouping: (required if no formula principal argument is given.)  a\n",
      "          factor specifying the class for each observation.\n",
      "\n",
      "   prior: the prior probabilities of class membership.  If unspecified,\n",
      "          the class proportions for the training set are used.  If\n",
      "          present, the probabilities should be specified in the order\n",
      "          of the factor levels.\n",
      "\n",
      "     tol: A tolerance to decide if a matrix is singular; it will reject\n",
      "          variables and linear combinations of unit-variance variables\n",
      "          whose variance is less than 'tol^2'.\n",
      "\n",
      "  subset: An index vector specifying the cases to be used in the\n",
      "          training sample.  (NOTE: If given, this argument must be\n",
      "          named.)\n",
      "\n",
      "na.action: A function to specify the action to be taken if 'NA's are\n",
      "          found.  The default action is for the procedure to fail.  An\n",
      "          alternative is 'na.omit', which leads to rejection of cases\n",
      "          with missing values on any required variable.  (NOTE: If\n",
      "          given, this argument must be named.)\n",
      "\n",
      "  method: '\"moment\"' for standard estimators of the mean and variance,\n",
      "          '\"mle\"' for MLEs, '\"mve\"' to use 'cov.mve', or '\"t\"' for\n",
      "          robust estimates based on a t distribution.\n",
      "\n",
      "      CV: If true, returns results (classes and posterior\n",
      "          probabilities) for leave-one-out cross-validation. Note that\n",
      "          if the prior is estimated, the proportions in the whole\n",
      "          dataset are used.\n",
      "\n",
      "      nu: degrees of freedom for 'method = \"t\"'.\n",
      "\n",
      "     ...: arguments passed to or from other methods.\n",
      "\n",
      "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
      "\n",
      "     The function tries hard to detect if the within-class covariance\n",
      "     matrix is singular. If any variable has within-group variance less\n",
      "     than 'tol^2' it will stop and report the variable as constant.\n",
      "     This could result from poor scaling of the problem, but is more\n",
      "     likely to result from constant variables.\n",
      "\n",
      "     Specifying the 'prior' will affect the classification unless\n",
      "     over-ridden in 'predict.lda'.  Unlike in most statistical\n",
      "     packages, it will also affect the rotation of the linear\n",
      "     discriminants within their space, as a weighted between-groups\n",
      "     covariance matrix is used. Thus the first few linear discriminants\n",
      "     emphasize the differences between groups with the weights given by\n",
      "     the prior, which may differ from their prevalence in the dataset.\n",
      "\n",
      "     If one or more groups is missing in the supplied data, they are\n",
      "     dropped with a warning, but the classifications produced are with\n",
      "     respect to the original set of levels.\n",
      "\n",
      "_\bV_\ba_\bl_\bu_\be:\n",
      "\n",
      "     If 'CV = TRUE' the return value is a list with components 'class',\n",
      "     the MAP classification (a factor), and 'posterior', posterior\n",
      "     probabilities for the classes.\n",
      "\n",
      "     Otherwise it is an object of class '\"lda\"' containing the\n",
      "     following components:\n",
      "\n",
      "   prior: the prior probabilities used.\n",
      "\n",
      "   means: the group means.\n",
      "\n",
      " scaling: a matrix which transforms observations to discriminant\n",
      "          functions, normalized so that within groups covariance matrix\n",
      "          is spherical.\n",
      "\n",
      "     svd: the singular values, which give the ratio of the between- and\n",
      "          within-group standard deviations on the linear discriminant\n",
      "          variables.  Their squares are the canonical F-statistics.\n",
      "\n",
      "       N: The number of observations used.\n",
      "\n",
      "    call: The (matched) function call.\n",
      "\n",
      "_\bN_\bo_\bt_\be:\n",
      "\n",
      "     This function may be called giving either a formula and optional\n",
      "     data frame, or a matrix and grouping factor as the first two\n",
      "     arguments.  All other arguments are optional, but 'subset=' and\n",
      "     'na.action=', if required, must be fully named.\n",
      "\n",
      "     If a formula is given as the principal argument the object may be\n",
      "     modified using 'update()' in the usual way.\n",
      "\n",
      "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
      "\n",
      "     Venables, W. N. and Ripley, B. D. (2002) _Modern Applied\n",
      "     Statistics with S._ Fourth edition.  Springer.\n",
      "\n",
      "     Ripley, B. D. (1996) _Pattern Recognition and Neural Networks_.\n",
      "     Cambridge University Press.\n",
      "\n",
      "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
      "\n",
      "     'predict.lda', 'qda', 'predict.qda'\n",
      "\n",
      "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
      "\n",
      "     Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),\n",
      "                        Sp = rep(c(\"s\",\"c\",\"v\"), rep(50,3)))\n",
      "     train <- sample(1:150, 75)\n",
      "     table(Iris$Sp[train])\n",
      "     ## your answer may differ\n",
      "     ##  c  s  v\n",
      "     ## 22 23 30\n",
      "     z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)\n",
      "     predict(z, Iris[-train, ])$class\n",
      "     ##  [1] s s s s s s s s s s s s s s s s s s s s s s s s s s s c c c\n",
      "     ## [31] c c c c c c c v c c c c v c c c c c c c c c c c c v v v v v\n",
      "     ## [61] v v v v v v v v v v v v v v v\n",
      "     (z1 <- update(z, . ~ . - Petal.W.))\n",
      "     "
     ]
    }
   ],
   "source": [
    "help(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lda_fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year < 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year < \n",
       "    2005)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "            LD1\n",
       "Lag1 -0.6420190\n",
       "Lag2 -0.5135293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(lda_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "smarket_2005 <- subset(Smarket, Year == 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lda_pred <- predict(lda_fit, smarket_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(lda_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>class</th><th scope=col>posterior.Down</th><th scope=col>posterior.Up</th><th scope=col>LD1</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>999</th><td>Up</td><td>0.4901792</td><td>0.5098208</td><td> 0.08293096</td></tr>\n",
       "\t<tr><th scope=row>1000</th><td>Up</td><td>0.4792185</td><td>0.5207815</td><td> 0.59114102</td></tr>\n",
       "\t<tr><th scope=row>1001</th><td>Up</td><td>0.4668185</td><td>0.5331815</td><td> 1.16723063</td></tr>\n",
       "\t<tr><th scope=row>1002</th><td>Up</td><td>0.4740011</td><td>0.5259989</td><td> 0.83335022</td></tr>\n",
       "\t<tr><th scope=row>1003</th><td>Up</td><td>0.4927877</td><td>0.5072123</td><td>-0.03792892</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 4\n",
       "\\begin{tabular}{r|llll}\n",
       "  & class & posterior.Down & posterior.Up & LD1\\\\\n",
       "  & <fct> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t999 & Up & 0.4901792 & 0.5098208 &  0.08293096\\\\\n",
       "\t1000 & Up & 0.4792185 & 0.5207815 &  0.59114102\\\\\n",
       "\t1001 & Up & 0.4668185 & 0.5331815 &  1.16723063\\\\\n",
       "\t1002 & Up & 0.4740011 & 0.5259989 &  0.83335022\\\\\n",
       "\t1003 & Up & 0.4927877 & 0.5072123 & -0.03792892\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 4\n",
       "\n",
       "| <!--/--> | class &lt;fct&gt; | posterior.Down &lt;dbl&gt; | posterior.Up &lt;dbl&gt; | LD1 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 999 | Up | 0.4901792 | 0.5098208 |  0.08293096 |\n",
       "| 1000 | Up | 0.4792185 | 0.5207815 |  0.59114102 |\n",
       "| 1001 | Up | 0.4668185 | 0.5331815 |  1.16723063 |\n",
       "| 1002 | Up | 0.4740011 | 0.5259989 |  0.83335022 |\n",
       "| 1003 | Up | 0.4927877 | 0.5072123 | -0.03792892 |\n",
       "\n"
      ],
      "text/plain": [
       "     class posterior.Down posterior.Up LD1        \n",
       "999  Up    0.4901792      0.5098208     0.08293096\n",
       "1000 Up    0.4792185      0.5207815     0.59114102\n",
       "1001 Up    0.4668185      0.5331815     1.16723063\n",
       "1002 Up    0.4740011      0.5259989     0.83335022\n",
       "1003 Up    0.4927877      0.5072123    -0.03792892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(lda_pred)[1:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      \n",
       "       Down  Up\n",
       "  Down   35  35\n",
       "  Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(lda_pred$class, smarket_2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(lda_pred$class == smarket_2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
